{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7282768,"sourceType":"datasetVersion","datasetId":4222864}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Python Implementation of binary classification of Brain Tumours and evaluating the model on different activation functions and Test train splits.**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dictionary of optimizers used for training**","metadata":{}},{"cell_type":"code","source":"optimizer_dict={\n  1: 'SGD',  \n  2: 'RMSprop',\n  3: 'Adam', \n  4: 'Adadelta',\n  5: 'Adagrad',\n  6: 'Adamax',    \n  7: 'Nadam'  \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image Pre-processing** ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport random\nimport cv2\nimport os\nimport matplotlib.image as mpimg\nfrom zipfile import ZipFile\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras.utils import load_img,img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n\n\ndata_path = os.listdir(r\"/kaggle/input/braintumour/BT/Testing\")\n\npath = r'/kaggle/input/braintumour/BT/Training'\nclasses = os.listdir(path)\nclasses\nfig = plt.gcf()\nfig.set_size_inches(16, 16)\n\nnotum_dir = os.path.join(r\"/kaggle/input/braintumour/BT/Training/notumor\")\ntum_dir = os.path.join(r\"/kaggle/input/braintumour/BT/Training/tumor\")\nnotum_names = os.listdir(notum_dir)\ntum_names = os.listdir(tum_dir)\n\npic_index = 210\n\nnotum_images = [os.path.join(notum_dir, fname)\n            for fname in notum_names[pic_index-8:pic_index]]\ntum_images = [os.path.join(tum_dir, fname)\n            for fname in tum_names[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(tum_images + notum_images):\n    sp = plt.subplot(4, 4, i+1)\n    sp.axis('Off')\n\nbase_dir = r'/kaggle/input/braintumour/BT/Training'\n\n\ntrain_datagen = image_dataset_from_directory(base_dir,\n                                                image_size=(200,200),\n                                                subset='training',\n                                                seed = 1,\n                                                validation_split=0.3,\n                                                batch_size= 32)\ntest_datagen = image_dataset_from_directory(base_dir,\n                                                image_size=(200,200),\n                                                subset='validation',\n                                                seed = 1,\n                                                validation_split=0.3,\n                                                batch_size= 32)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Definition**","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    layers.Conv2D(256, (2, 2), activation='relu', input_shape=(200, 200, 3)),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (2, 2), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (2, 2), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(32, (2, 2), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.1),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()\n'''optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='Adam',\n    metrics=['accuracy']\n)\nhistory = model.fit(train_datagen,\n        epochs=2,\n        validation_data=test_datagen)\ndata_columns=[\"Optimizer\", \"Epoch\", \"Training Loss\",\" Training Accuracy\",\"Validation Loss\",\"Validation Accuracy\"]\ndata_df=pd.DataFrame(columns=data_columns)\nhistory_df = pd.DataFrame(history.history)\ndata_df = pd.DataFrame(history_df.values, columns=data_df.columns[2:])\ndata_df.to_csv('mycsvfile.csv', mode='a', header=False, index=False)\n\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nplt.xlim(0, 10)\nplt.ylim(0, 1)\nplt.show()\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dot_img_file = 'model_1.png'\ntf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Performance Metrics Evaluation**","metadata":{}},{"cell_type":"code","source":"with open('modelsummary.txt', 'w') as f:\n\n    model.summary(print_fn=lambda x: f.write(x + '\\n'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_recall_precision_sensitivity_specificity(confusion_matrix):\n    true_positives = confusion_matrix[1][1]\n    false_positives = confusion_matrix[0][1]\n    false_negatives = confusion_matrix[1][0]\n    true_negatives = confusion_matrix[0][0]\n\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n    sensitivity = recall\n    specificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) != 0 else 0\n\n    return recall, precision, sensitivity, specificity\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Training and Results**","metadata":{}},{"cell_type":"code","source":"import csv\nimport pandas as pd\nfrom keras import backend\nfrom sklearn.metrics import confusion_matrix\ncsv_columns = [\"Optimizer\", \"Epoch\", \"Training Loss\", \"Training Accuracy\", \"Validation Loss\", \"Validation Accuracy\", \"Sensitivity\", \"Specificity\", \"Precision\", \"Recall\"]\ncsv_df = pd.DataFrame(columns=csv_columns)\ncsv_df.to_csv('Sigmoid 70-30.csv', index=False)\n\nmodel.compile(\nloss='binary_crossentropy',\noptimizer=optimizer_dict[3],\nmetrics=['accuracy']\n        )\nmodel.save_weights('model.h5')\nprint(\"Currently Training for \",20,\"Epoch(s).\")\nhistory = model.fit(train_datagen,\nepochs=20,\nvalidation_data=test_datagen)\nhistory_df = pd.DataFrame(history.history)\ntest_generator = test_datagen.unbatch().batch(900000000)\ntest_images, test_labels = next(iter(test_generator))\npredictions = model.predict(test_images)\npredicted_classes = np.round(predictions)\ncm = confusion_matrix(test_labels, predicted_classes)\nrecall, precision, sensitivity, specificity = calculate_recall_precision_sensitivity_specificity(cm)\nsave_path=\"model\"+str(optimizer_dict[i])+\".h5\"\nmodel.save_weights(save_path)\nmodel.load_weights('model.h5') \nprint(\"ALL DONE\")","metadata":{"_kg_hide-input":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def swap_diagonal(matrix):\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        print(\"Error: Input matrix must be a 2x2 matrix.\")\n        return None\n\n    temp = matrix[0][0]\n    matrix[0][0] = matrix[1][1]\n    matrix[1][1] = temp\n\n    return matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=swap_diagonal(cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = ['Tumour', 'NoTumour']\n\n# Create a seaborn heatmap\nsns.set(font_scale=1.4)  # Adjust font scale if necessary\nplt.figure(figsize=(6, 4))  # Adjust figure size if necessary\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16},\n            xticklabels=labels, yticklabels=labels)\n\n# Add labels and title\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\n\n# Show plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model = tf.keras.models.Sequential([\n    layers.Conv2D(256, (2, 2), activation='relu', input_shape=(200, 200, 3)),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (2, 2), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (2, 2), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(32, (2, 2), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.1),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(1, activation='sigmoid')\n])\n\nfor i in optimizer_dict:\n    path='model'+optimizer_dict[i]+'.h5'\n    my_model.load_weights(path)\n    max_weight = float('-inf')\n    min_weight = float('inf')\n\n    # Iterate through each layer of the model\n    for layer in model.layers:\n        # Get the weights of the layer\n        weights = layer.get_weights()\n        if len(weights) > 0:\n            # Iterate through each weight matrix in the layer\n            for weight_matrix in weights:\n                # Update the maximum and minimum weights\n                max_weight_in_matrix = tf.reduce_max(weight_matrix)\n                min_weight_in_matrix = tf.reduce_min(weight_matrix)\n                if max_weight_in_matrix > max_weight:\n                    max_weight = max_weight_in_matrix\n                if min_weight_in_matrix < min_weight:\n                    min_weight = min_weight_in_matrix\n\n    print(\"Range for 70-30 split for the \",optimizer_dict[i], max_weight,\"-\", min_weight)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# **Blob Detection Using Open CV**","metadata":{}},{"cell_type":"code","source":"\nimgExtension = [\"png\", \"jpeg\", \"jpg\"] #Image Extensions to be chosen from\nallImages = list()\n\n\n\ndef chooseRandomImage(directory=r\"/kaggle/input/braintumour/BT/Testing\"):\n    for img in os.listdir(directory): #Lists all files\n        ext = img.split(\".\")[len(img.split(\".\")) - 1]\n        if (ext in imgExtension):\n            allImages.append(img)\n    choice = random.randint(0, len(allImages) - 1)\n    chosenImage = allImages[choice] #Do Whatever you want with the image file\n    return chosenImage\n\nrandomImage = chooseRandomImage()\n\n\n#Input image\ntest_image = load_img('/kaggle/input/braintumour/BT/Testing//'+randomImage,target_size=(200,200))\npath='/kaggle/input/braintumour/BT/Testing//'+randomImage\n\ntest_image = img_to_array(test_image)\ntest_image = np.expand_dims(test_image,axis=0)\n\n\nresult = model.predict(test_image)\n\n#Mapping result array with the main name list\ni=0\nif(result>=0.5):\n    print(\" Tumor found\")\nelse:\n    print(\"No tumor found\")\nimg = cv2.imread(path)  \n\n# Convert to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Apply Gaussian blur to reduce noise  \nblur = cv2.GaussianBlur(gray, (5,5), 0)\n\n# Perform blob detection\nparams = cv2.SimpleBlobDetector_Params()\nparams.filterByColor = True\nparams.blobColor = 255 # Detect brighter white blobs\n\n# Filter by circularity to detect rounder blobs\nparams.filterByCircularity = True \nparams.minCircularity = 0.2\n\n# Filter by convexity to detect convex blobs\nparams.filterByConvexity = True\nparams.minConvexity = 0.2\n\n# Create blob detector with custom params\ndetector = cv2.SimpleBlobDetector_create(params)\n\n# Detect brighter blobs \nkeypoints = detector.detect(blur)\nparams.filterByArea = True\nparams.minArea = 500\nparams.maxArea = 2000\n\n\n\ndetector = cv2.SimpleBlobDetector_create(params)\nkeypoints = detector.detect(blur)\n\n# Draw boxes around detected blobs\nfor k in keypoints:\n    x, y, r = int(k.pt[0]), int(k.pt[1]), int(k.size/2)\n    cv2.rectangle(img, (x-r, y-r), (x+r, y+r), (0,0,255), 2)\n\n# Show result\nplt.imshow(img) \n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}